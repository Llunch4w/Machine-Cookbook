{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**主题建模**    \n",
    "指识别文本数据隐藏模式的过程，其目的是发现一组文档的隐藏主题结构。     \n",
    "主题建模可以更好地组织文档，以便对这些文档进行分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**工作原理**    \n",
    "主题建模通过识别文档中最有意义，最能表征主题的词来实现主题分类，这些单词往往可以确定主题的内容      \n",
    "- 使用正则表达式标记器是因为只需要那些没有标点或者其他标记的单词\n",
    "- 停用词去除可以减小一些常用词('is','the')的噪声干扰。\n",
    "- 之后对单词做词干提取，以获取其原形\n",
    "\n",
    "本例中用到了隐含狄利克雷分布技术来构建主题建模。\n",
    "隐含狄利克雷分布将文档表示成不同主题的混合，这些主题可以“吐出”单词，\n",
    "这些“吐出”的单词是有一定概率的。隐含狄利克雷分布的目标是找到这些主题。        \n",
    "隐含狄利克雷分布是一个生出主题的模型，该模型视图找到所有主题，而所有主题又负责生成给定主题的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most contributing words to the topics:\n",
      "\n",
      "Topic 0 ==> 0.054*\"need\" + 0.033*\"order\" + 0.033*\"polici\" + 0.033*\"develop\"\n",
      "\n",
      "Topic 1 ==> 0.059*\"need\" + 0.036*\"parti\" + 0.036*\"messag\" + 0.036*\"make\"\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 加载数据\n",
    "def txt_read(filename):\n",
    "    x_data = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip()\n",
    "            x_data.append(data)\n",
    "\n",
    "    return x_data\n",
    "\n",
    "# 定义一个预处理文本类\n",
    "# 处理相应对象，并从输入文本中提取相关的特征\n",
    "class Preprocessor(object):\n",
    "    # 对各种操作进行初始化\n",
    "    def __init__(self):\n",
    "        # 创建正则表达式解析器\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # 获取停用词列表\n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "        # 创建Snowball词干提取器\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        \n",
    "    # 标记解析、移除停用词、词干提取\n",
    "    def process(self, input_text):\n",
    "        # 标记解析\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "        # 移除停用词\n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "        # 词干提取\n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "        return tokens_stemmed\n",
    "    \n",
    "# 输入数据的文件\n",
    "input_file = 'data_topic_modeling.txt'\n",
    "data = txt_read(input_file)\n",
    "# 创建预处理对象\n",
    "preprocessor = Preprocessor()\n",
    "# 创建一组经过预处理的文档\n",
    "processed_tokens = [preprocessor.process(x) for x in data]\n",
    "# 创建基于标记文档的词典\n",
    "dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "# 创建文档-词矩阵\n",
    "corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "# 假定文本可以分成两个主题。我们将用隐含狄利克雷分布做主题建模\n",
    "# 基于刚刚创建的语料库生成LDA模型\n",
    "num_topics = 2\n",
    "num_words = 4\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, \n",
    "        num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "# 识别出两个主题后，可以看到它是如何将两个主题分开来看的\n",
    "print(\"\\nMost contributing words to the topics:\")\n",
    "for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "    print(\"\\nTopic\", item[0], \"==>\", item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
