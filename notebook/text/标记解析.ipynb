{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLP（Natural Language Processing,自然语言处理）是现代人工智能系统不可分割的一部分。计算机擅长用有限的多样性来理解结构死板的数据。 \n",
    "然而，但我们处理非结构化的自由文本时，就会变得很困难。      \n",
    "为了解决这个问题，基于机器学习的NLP应运而生。    \n",
    "NLP最常用的领域包括搜索引擎，情感分析，主题建模，词性标注，实体分析等。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标记分析是将文本分割成一组有意义的片段的过程。这些片段被称作标记，例如将一段文字分割成单词或者句子。根据手头的任务需要，\n",
    "可以自定义将输入的文本分割成有意义的标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n",
      "\n",
      "Word tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n",
      "\n",
      "Word punct tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\"\n",
    "\n",
    "# 句子解析\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize_list)\n",
    "\n",
    "# 基本单词解析\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# 如果需要将标点符号保留到不同的句子标记中，可以用WordPunct标记解析器\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(\"\\nWord punct tokenizer:\")\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取文本数据的词干**   \n",
    "处理文本文档时，可能会碰到单词的不同形式。    \n",
    "例如play，有play,plays,playing,player,playing等等，这些事具有同样含义的单词家族。    \n",
    "在文本分析中，提取这些单词的原型非常有用。它有助于我么提取一些统计信息来分析整个文本。    \n",
    "词干提取的目标是将不同词形的单词都变成其原形。词干提取使用启发式处理方法截取单词的尾部，以提取单词的原形。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# 定义一些单词进行词干提取\n",
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
    "\n",
    "# 定义一个稍后会用到的词干提取器列表\n",
    "# 对比不同的词干提取器对象\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "# 初始化3个词干提取器对象\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')\n",
    "# 为了以整齐的表格形式将输出数据打印出来设定的正确格式\n",
    "formatted_row = '{:>16}' * (len(stemmers) + 1) # >向右对齐，<向左对齐\n",
    "print('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word), \n",
    "            stemmer_lancaster.stem(word), stemmer_snowball.stem(word)]\n",
    "    print(formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "严格程度 Lancaster > Snowball > Porter,一般采用Snowball词干提取器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一节中，可以看到用词根还原得到的单词原型并不是有意义的。    \n",
    "词形还原通过对单词进行词汇和语法分析来实现，可以解决这个问题。     \n",
    "下面介绍用词形还原的方法还原文本的基本形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      "\n",
      "                   table                   table                   table\n",
      "                probably                probably                probably\n",
      "                  wolves                    wolf                  wolves\n",
      "                 playing                 playing                    play\n",
      "                      is                      is                      be\n",
      "                     dog                     dog                     dog\n",
      "                     the                     the                     the\n",
      "                 beaches                   beach                   beach\n",
      "                grounded                grounded                  ground\n",
      "                  dreamt                  dreamt                   dream\n",
      "                envision                envision                envision\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\n",
    "\n",
    "# 比较两个不同的词形还原器\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "\n",
    "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
    "print('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "for word in words:\n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'),\n",
    "           lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print(formatted_row.format(word, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用分块的方法划分文本**     \n",
    "分块是指基于任意随机条件将输入文本分割成块。     \n",
    "与标记解析不同的是，分块没有条件约束，分块的结果不需要有实际意义    \n",
    "当处理非常大的文本文档时，就需要将文本进行分块，以便于下一步分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 将文本分割成块\n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "\n",
    "    output.append(' '.join(cur_words)) # 剩余部分\n",
    "\n",
    "    return output \n",
    "\n",
    "\n",
    "# 从布朗语料库加载数据(Brown corpus)加载数据，用到前10000个单词\n",
    "data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "# 定义每块包含的单词数目 \n",
    "num_words = 1700\n",
    "\n",
    "text_chunks = splitter(data, num_words)\n",
    "\n",
    "print(\"Number of text chunks =\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词袋模型**     \n",
    "如果需要处理包含数百万单词的文本文档，需要将其转化为某种数值表示的形式，以便让机器用这些数据来学习算法。\n",
    "这些算法需要数值数据，以便可以对这些数据进行分析，并输出有用信息。     \n",
    "这里需要用到词袋。词袋是从所有文档的所有单词中学习词汇的模型。词袋通过构建文档中所有单词的直方图来对每篇文档进行建模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n",
      "\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "data = ' '.join(brown.words()[:10000])\n",
    "num_words = 2000\n",
    "\n",
    "chunks = []\n",
    "counter = 0\n",
    "\n",
    "text_chunks = splitter(data, num_words)\n",
    "\n",
    "# 创建一个基于这些文本块的词典\n",
    "for text in text_chunks:\n",
    "    chunk = {'index': counter, 'text': text}\n",
    "    chunks.append(chunk)\n",
    "    counter += 1\n",
    "\n",
    "# 提取文档-词矩阵，文档-词矩阵激励了文档中每个单词出现的频次\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 定义对象\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "# 提取文档-词矩阵\n",
    "doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "# 从vectorizer对象中提取词汇\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "print(\"\\nVocabulary:\")\n",
    "print(vocab)\n",
    "\n",
    "# 打印文档-词矩阵\n",
    "print(\"\\nDocument term matrix:\")\n",
    "chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "print('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
    "for word, item in zip(vocab, doc_term_matrix.T):\n",
    "    # 'item' 是压缩的系数矩阵结构\n",
    "    output = [str(x) for x in item.data]\n",
    "    print(formatted_row.format(word, *output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
