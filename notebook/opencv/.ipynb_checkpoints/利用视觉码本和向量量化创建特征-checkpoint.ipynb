{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了创建一个目标识别系统，需要从每张图像中提取特征向量。每张图像需要有一个标识标志，以用于匹配。     \n",
    "用一个叫**视觉码本**的概念来创建图像识别标志。在训练数据集中，这个码本基本上是一个字典，用于提出关于图像的描述。     \n",
    "我们用**向量量化**方法将很多特征点进行聚类并得出中心点。这些中心点作为视觉码本的元素。    \n",
    "\n",
    "本例提供了包含3个类的示例训练数据集，每一类包含20幅图像，这些图像可以在[网址](http://www.vision.caltech.edu/html-files/archive.html)下载  \n",
    "\n",
    "为了创建一个健壮的目标识别系统，需要数万幅图像。该领域有一个非常著名的数据集叫Caltech256，它包括256类图像，每一类都包含上千幅示例图像。可以在[网址](http://www.vision.caltech.edu/Image_Datasets/Caltech256)下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class StarFeatureDetector(object):\n",
    "    def __init__(self):\n",
    "        self.detector = cv2.xfeatures2d.StarDetector_create()\n",
    "\n",
    "    def detect(self, img):\n",
    "        return self.detector.detect(img)\n",
    "    \n",
    "class FeatureBuilder(object):\n",
    "    # 定义一个从输入图像提取特征的方法\n",
    "    # 用Star检测器获得关键点，然后用SIFT提取这些位置的描述信息\n",
    "    def extract_features(self, img):\n",
    "        keypoints = StarFeatureDetector().detect(img)\n",
    "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
    "        return feature_vectors\n",
    "    # 从描述信息中提取中心点\n",
    "    def get_codewords(self, input_map, scaling_size, max_samples=12):\n",
    "        keypoints_all = []\n",
    "        \n",
    "        count = 0\n",
    "        cur_class = ''\n",
    "        # 每幅图像都会产生大量的描述信息。这里将仅用一小部分图像，因为这些中心点并不会发生很大的改变\n",
    "        for item in input_map:\n",
    "            if count >= max_samples:\n",
    "                if cur_class != item['object_class']:\n",
    "                    count = 0\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count == max_samples:\n",
    "                print(\"Built centroids for\", item['object_class'])\n",
    "            # 提取当前标签\n",
    "            cur_class = item['object_class']\n",
    "            # 读取图像并调整其大小\n",
    "            img = cv2.imread(item['image_path'])\n",
    "            img = resize_image(img, scaling_size)\n",
    "            # 设置维度数为128并提取特征\n",
    "            num_dims = 128\n",
    "            feature_vectors = self.extract_features(img)\n",
    "            keypoints_all.extend(feature_vectors) \n",
    "        # 用向量量化来量化特征点\n",
    "        kmeans, centroids = BagOfWords().cluster(keypoints_all)\n",
    "        return kmeans, centroids\n",
    "    \n",
    "# 定义一个类来处理词袋模型和向量量化\n",
    "class BagOfWords(object):\n",
    "    def __init__(self, num_clusters=32):\n",
    "        self.num_dims = 128\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_retries = 10\n",
    "    # 定义一个方法来量化数据点。下面将用k-means聚类来实现\n",
    "    def cluster(self, datapoints):\n",
    "        kmeans = KMeans(self.num_clusters, \n",
    "                        n_init=max(self.num_retries, 1),\n",
    "                        max_iter=10, tol=1.0)\n",
    "        # 提取中心点\n",
    "        res = kmeans.fit(datapoints)\n",
    "        centroids = res.cluster_centers_\n",
    "        return kmeans, centroids\n",
    "    # 定义一个方法来归一化数据\n",
    "    def normalize(self, input_data):\n",
    "        sum_input = np.sum(input_data)\n",
    "\n",
    "        if sum_input > 0:\n",
    "            return input_data / sum_input\n",
    "        else:\n",
    "            return input_data\n",
    "    # 定义一个方法来获得特征向量\n",
    "    def construct_feature(self, img, kmeans, centroids):\n",
    "        keypoints = StarFeatureDetector().detect(img)\n",
    "        keypoints, feature_vectors = compute_sift_features(img, keypoints)\n",
    "        labels = kmeans.predict(feature_vectors)\n",
    "        feature_vector = np.zeros(self.num_clusters)\n",
    "\n",
    "        for i, item in enumerate(feature_vectors):\n",
    "            feature_vector[labels[i]] += 1\n",
    "\n",
    "        feature_vector_img = np.reshape(feature_vector, \n",
    "                ((1, feature_vector.shape[0])))\n",
    "        return self.normalize(feature_vector_img)\n",
    "\n",
    "# 获取特征\n",
    "def get_feature_map(input_map, kmeans, centroids, scaling_size):\n",
    "    feature_map = []\n",
    "     \n",
    "    for item in input_map:\n",
    "        temp_dict = {}\n",
    "        temp_dict['object_class'] = item['object_class']\n",
    "    \n",
    "        print(\"Extracting features for\", item['image_path'])\n",
    "        img = cv2.imread(item['image_path'])\n",
    "        img = resize_image(img, scaling_size)\n",
    "\n",
    "        temp_dict['feature_vector'] = BagOfWords().construct_feature(\n",
    "                    img, kmeans, centroids)\n",
    "\n",
    "        if temp_dict['feature_vector'] is not None:\n",
    "            feature_map.append(temp_dict)\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "# 提取SIFT特征\n",
    "def compute_sift_features(img, keypoints):\n",
    "    if img is None:\n",
    "        raise TypeError('Invalid input image')\n",
    "\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = cv2.xfeatures2d.SIFT_create().compute(img_gray, keypoints)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# 将图像最短的那个维度转化为new_size,另一个维度等比例变换\n",
    "def resize_image(input_img, new_size):\n",
    "    h, w = input_img.shape[:2]\n",
    "    scaling_factor = new_size / float(h)\n",
    "\n",
    "    if w < h:\n",
    "        scaling_factor = new_size / float(w)\n",
    "\n",
    "    new_shape = (int(w * scaling_factor), int(h * scaling_factor))\n",
    "    return cv2.resize(input_img, new_shape) \n",
    "\n",
    "# 加载图片并按文件夹名字（具体分类）做标记\n",
    "def load_training_data(input_folder):\n",
    "    training_data = []\n",
    "\n",
    "    if not os.path.isdir(input_folder):\n",
    "        raise IOError(\"The folder \" + input_folder + \" doesn't exist\")\n",
    "        \n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for filename in (x for x in files if x.endswith('.jpg')):\n",
    "            filepath = root + '/' + filename\n",
    "            object_class = filepath.split('/')[-2]\n",
    "            training_data.append({'object_class': object_class, \n",
    "                'image_path': filepath})\n",
    "                    \n",
    "    return training_data\n",
    "\n",
    "if __name__=='__main__':\n",
    "    codebook_file = \"codebook.pkl\"\n",
    "    feature_map_file = \"feature_map.pkl\"\n",
    "    data_folder = \"./training_images/\"\n",
    "    scaling_size = 200\n",
    "    \n",
    "    # 加载训练数据\n",
    "    training_data = load_training_data(data_folder)\n",
    "\n",
    "    # 建立虚拟字典\n",
    "    print(\"====== Building visual codebook ======\")\n",
    "    kmeans, centroids = FeatureBuilder().get_codewords(training_data, scaling_size)\n",
    "    if codebook_file:\n",
    "        with open(codebook_file, 'wb') as f:\n",
    "            pickle.dump((kmeans, centroids), f)\n",
    "    \n",
    "    # 从输入图像中提取特征\n",
    "    print(\"\\n====== Building the feature map ======\")\n",
    "    feature_map = get_feature_map(training_data, kmeans, centroids, scaling_size)\n",
    "    if feature_map_file:\n",
    "        with open(feature_map_file, 'wb') as f:\n",
    "            pickle.dump(feature_map, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
